{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3426b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q vosk gTTS faster-whisper pydub soundfile sentencepiece\n",
    "!apt -qq install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3577b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Single-Cell STT Pipeline (Colab)\n",
    "# TTS → WAV → Vosk → Whisper\n",
    "# ================================\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "from IPython.display import Audio,display\n",
    "import os, wave, json, subprocess\n",
    "from gtts import gTTS\n",
    "from vosk import Model,KaldiRecognizer\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "\n",
    "print(\"please upload files: \")\n",
    "up = files.upload()\n",
    "\n",
    "if up:\n",
    "  audio = list(up.keys())[0]\n",
    "else:\n",
    "  text_input = \"This audio is generated from text for testing purpose\"\n",
    "  tts = gTTS(text =text_input,lang='en')\n",
    "  tts.save('tts.mp3')\n",
    "print(\"Generated audio from text:\", text_input)\n",
    "Audio(\"tts.mp3\")\n",
    "\n",
    "\n",
    "# 2) Convert → WAV 16khz mono\n",
    "wav = \"audio_tts.wav\"\n",
    "subprocess.run([\"ffmpeg\",\"-y\",\"-i\",'tts.mp3',\"-ar\",\"16000\",\"-ac\",\"1\",wav],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"Converted to audio.wav (16k mono)\")\n",
    "Audio(wav)\n",
    "\n",
    "# 3) --- VOSK STT ---\n",
    "if not os.path.exists(\"vosk-model\"):\n",
    "    !wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -O model.zip\n",
    "    !unzip -q model.zip\n",
    "    !mv vosk-model-small-en-us-0.15 vosk-model\n",
    "    !rm model.zip\n",
    "\n",
    "wf = wave.open(wav,'rb')\n",
    "rec = KaldiRecognizer(Model(\"vosk-model\"),16000)\n",
    "\n",
    "vosk_text = \"\"\n",
    "\n",
    "while True:\n",
    "  data = wf.readframes(4000)\n",
    "  if not data: break\n",
    "  if rec.AcceptWaveform(data):\n",
    "    vosk_text+= json.loads(rec.Result()).get('text',\" \")+ \" \"\n",
    "vosk_text += json.loads(rec.FinalResult()).get('text',\" \")+ \" \"\n",
    "\n",
    "\n",
    "# 4) --- whisper stt---\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "whisper = WhisperModel('small',device=device)\n",
    "\n",
    "segments,_ = whisper.transcribe(wav)\n",
    "whisper_text = \" \".join([s.text for s in segments]).strip()\n",
    "\n",
    "# 5) Results\n",
    "print(\"\\n===== VOSK OUTPUT =====\")\n",
    "print(vosk_text)\n",
    "\n",
    "print(\"\\n===== WHISPER OUTPUT =====\")\n",
    "print(whisper_text)\n",
    "\n",
    "print(\"\\n===== ORIGINAL TEXT =====\")\n",
    "print(text_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f26e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11090d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p librispeech_sample\n",
    "!wget -q https://www.openslr.org/resources/12/dev-clean.tar.gz -O dev-clean.tar.gz\n",
    "!tar -xzf dev-clean.tar.gz --directory librispeech_sample --wildcards \"*.flac\"\n",
    "\n",
    "print(\"Sample files:\")\n",
    "!find librispeech_sample -name \"*.flac\" | head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b745c",
   "metadata": {},
   "source": [
    "Convert dataset audio (FLAC → WAV, 16k mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "\n",
    "source = \"librispeech_sample/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\"\n",
    "target = \"sample_1272.wav\"\n",
    "\n",
    "subprocess.run([\"ffmpeg\", \"-y\", \"-i\", source, \"-ar\", \"16000\", \"-ac\", \"1\", target])\n",
    "\n",
    "print(\"Converted to:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c65909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "os.makedirs(\"synthetic_dataset\", exist_ok=True)\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, welcome to the speech recognition test.\",\n",
    "    \"This is a synthetic dataset created using text to speech.\",\n",
    "    \"Speech models must be evaluated for accuracy.\",\n",
    "    \"Different speakers and accents should be tested.\",\n",
    "    \"Background noise can affect transcription quality.\",\n",
    "    \"We will benchmark whisper and vosk models.\",\n",
    "    \"This sentence is intentionally longer to test robustness.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"Thank you for participating in this project.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sentences):\n",
    "    tts = gTTS(text=text, lang=\"en\")\n",
    "    path = f\"synthetic_dataset/audio_{i}.mp3\"\n",
    "    tts.save(path)\n",
    "\n",
    "print(\"Synthetic dataset created:\")\n",
    "!ls -1 synthetic_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb6511",
   "metadata": {},
   "source": [
    "Create a synthetic dataset automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "os.makedirs(\"synthetic_dataset\", exist_ok=True)\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, welcome to the speech recognition test.\",\n",
    "    \"This is a synthetic dataset created using text to speech.\",\n",
    "    \"Speech models must be evaluated for accuracy.\",\n",
    "    \"Different speakers and accents should be tested.\",\n",
    "    \"Background noise can affect transcription quality.\",\n",
    "    \"We will benchmark whisper and vosk models.\",\n",
    "    \"This sentence is intentionally longer to test robustness.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"Thank you for participating in this project.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sentences):\n",
    "    tts = gTTS(text=text, lang=\"en\")\n",
    "    path = f\"synthetic_dataset/audio_{i}.mp3\"\n",
    "    tts.save(path)\n",
    "\n",
    "print(\"Synthetic dataset created:\")\n",
    "!ls -1 synthetic_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "os.makedirs(\"synthetic_dataset_new\", exist_ok=True)\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, welcome to the speech recognition test.\\nPlease speak clearly so we can capture accurate audio.\\nOur system logs every utterance for downstream evaluation.\",\n",
    "\n",
    "    \"This is a synthetic dataset created using text to speech.\\nThe generated audio simulates natural cadence and pacing.\\nWe use this to stress-test the model under controlled conditions.\",\n",
    "\n",
    "    \"Speech models must be evaluated for accuracy.\\nPrecision, recall, and word error rate drive our performance metrics.\\nConsistent benchmarking helps us optimize the inference stack.\",\n",
    "\n",
    "    \"Different speakers and accents should be tested.\\nDiverse demographics strengthen model generalization.\\nGlobal deployment requires multilingual robustness.\",\n",
    "\n",
    "    \"Background noise can affect transcription quality.\\nWe simulate environments like cafés, offices, and transit stations.\\nNoise-suppression algorithms are applied for comparative analysis.\",\n",
    "\n",
    "    \"We will benchmark whisper and vosk models.\\nBoth models have unique strengths across latency and accuracy.\\nThe evaluation pipeline standardizes all test conditions.\",\n",
    "\n",
    "    \"This sentence is intentionally longer to test robustness.\\nLong-form utterances help identify segmentation issues.\\nWe monitor drift in recognition accuracy over extended speech.\",\n",
    "\n",
    "    \"The quick brown fox jumps over the lazy dog.\\nThis classic pangram ensures full alphabet coverage.\\nIt remains a staple in acoustic testing workflows.\",\n",
    "\n",
    "    \"Artificial intelligence is transforming industries.\\nAutomation pipelines are redefining operational efficiency.\\nNext-gen systems deliver scalable, real-time insights.\",\n",
    "\n",
    "    \"Thank you for participating in this project.\\nYour contributions support continuous model improvement.\\nWe appreciate your role in advancing speech technology.\"\n",
    "]\n",
    "\n",
    "\n",
    "for i, text in enumerate(sentences):\n",
    "    tts = gTTS(text=text, lang=\"en\")\n",
    "    path = f\"synthetic_dataset_new/audio_{i}.mp3\"\n",
    "    tts.save(path)\n",
    "\n",
    "print(\"Synthetic dataset created:\")\n",
    "!ls -1 synthetic_dataset_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd97d6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
